{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arghyadeep\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from client import DiffbotClient,DiffbotCrawl\n",
    "#from config import API_TOKEN   #file doesn not exist\n",
    "import pprint\n",
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models, similarities\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "\n",
    "class MyPrettyPrinter(pprint.PrettyPrinter):\n",
    "    def format(self, object, context, maxlevels, level):\n",
    "        if isinstance(object, unicode):\n",
    "            return (object.encode('ascii'), True, False)\n",
    "        return pprint.PrettyPrinter.format(self, object, context, maxlevels, level)\n",
    "\n",
    "api = \"article\"\n",
    "token = \"d40071eafe3464dd1699f98240a22f10\"\n",
    "#print (\"Calling article API endpoint on the url: http://shichuan.github.io/javascript-patterns/...\\n\")\n",
    "diffbot = DiffbotClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://en.wikipedia.org/wiki/Biryani\",\n",
    "       \"https://en.wikipedia.org/wiki/Lasagne\",\n",
    "       \"https://en.wikipedia.org/wiki/Shawarma\",\n",
    "       \"https://en.wikipedia.org/wiki/Hamburger\",    #####food\n",
    "       \"https://en.wikipedia.org/wiki/Noodle\",\n",
    "       \"https://en.wikipedia.org/wiki/Coq_au_vin\",\n",
    "       \"https://en.wikipedia.org/wiki/Risotto\",\n",
    "       \"https://en.wikipedia.org/wiki/Greek_salad\",\n",
    "       \n",
    "       \"https://en.wikipedia.org/wiki/Michael_Jackson\",\n",
    "       \"https://en.wikipedia.org/wiki/Jimi_Hendrix\",\n",
    "       \"https://en.wikipedia.org/wiki/Brad_Pitt\",\n",
    "       \"https://en.wikipedia.org/wiki/Shah_Rukh_Khan\",\n",
    "       \"https://en.wikipedia.org/wiki/Cristiano_Ronaldo\",    ####entertainment\n",
    "       \"https://en.wikipedia.org/wiki/Rowan_Atkinson\",\n",
    "       \"https://en.wikipedia.org/wiki/A._R._Rahman\",\n",
    "       \n",
    "       \"https://en.wikipedia.org/wiki/NASA\",\n",
    "       \"https://en.wikipedia.org/wiki/Indian_Space_Research_Organisation\",\n",
    "        \"https://en.wikipedia.org/wiki/Apple_Inc.\",\n",
    "        \"https://en.wikipedia.org/wiki/Google\",     ####technology\n",
    "        \"https://en.wikipedia.org/wiki/Tesla\",\n",
    "        \"https://en.wikipedia.org/wiki/GlaxoSmithKline\",\n",
    "        \"https://en.wikipedia.org/wiki/Audi\",\n",
    "        \n",
    "       \n",
    "       \"https://en.wikipedia.org/wiki/Football\",\n",
    "       \"https://en.wikipedia.org/wiki/Hockey\",\n",
    "       \"https://en.wikipedia.org/wiki/Cricket\",\n",
    "       \"https://en.wikipedia.org/wiki/Archery\",      ####sports\n",
    "       \"https://en.wikipedia.org/wiki/Golf\",\n",
    "       \"https://en.wikipedia.org/wiki/American_Football\",\n",
    "       \"https://en.wikipedia.org/wiki/Tennis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from urls using diffbot api and save text data in text files\n",
    "for i in range(len(urls)):\n",
    "    filename = \"out-00\" + str(i) + \".txt\"\n",
    "    response = diffbot.request(urls[i],token,api)\n",
    "    with open(filename, \"w\", encoding = \"utf-8\") as text_file:\n",
    "        text_file.write(response['objects'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from text files and get data in an array\n",
    "texts = []\n",
    "for i in range(len(urls)):\n",
    "    filename = \"out-00\" + str(i) + \".txt\"\n",
    "    wrapper = open(filename, \"r\", encoding=\"utf-8\")\n",
    "    text = wrapper.readlines()\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13287\n"
     ]
    }
   ],
   "source": [
    "#filter all texts with lemmatizer, removing numbers, lower all words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all_words = set()\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "refined_texts = []\n",
    "for docs in texts:\n",
    "    documents = []\n",
    "    for words in docs:\n",
    "        temp = words.split()\n",
    "        for word in temp:\n",
    "            refine1 = word.lower()\n",
    "            refine2 = lemmatizer.lemmatize(refine1)\n",
    "            if refine2.isalpha() and refine2 not in stop_words:\n",
    "                all_words.add(refine2)\n",
    "                refine3 = refine2.lower()\n",
    "                documents.append(refine3)\n",
    "    refined_texts.append(' '.join(documents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to count ocurrence of each word accross number of documents (inverse document frequency)\n",
    "counts = {}\n",
    "for word in all_words:\n",
    "    for doc in refined_texts:\n",
    "        if word in doc and word in counts:\n",
    "            counts[word] += 1\n",
    "        elif word in doc and word not in counts:\n",
    "            counts[word] = 1\n",
    "c = Counter(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with high frequency ocurring accross multiple documents and also remove words ocurring in very few documents\n",
    "refined_words = []\n",
    "for word in dict(c):\n",
    "    if dict(c)[word] < 15:\n",
    "        refined_words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refine all documents with refined words\n",
    "final_refined_texts = []\n",
    "final_refined_texts_lists = []\n",
    "for doc in refined_texts:\n",
    "    temp = []\n",
    "    for  word in doc.split():\n",
    "        if word in refined_words:\n",
    "            temp.append(word)\n",
    "    final_refined_texts.append(\" \".join(temp))\n",
    "    final_refined_texts_lists.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary and corpus for lda model\n",
    "dictionary = corpora.Dictionary(final_refined_texts_lists)\n",
    "corpus = [dictionary.doc2bow(text) for text in final_refined_texts_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train lda model\n",
    "lda = models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, iterations=1000, num_topics = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.008*\"audi\" + 0.007*\"hamburger\" + 0.007*\"noodle\" + 0.005*\"space\" + 0.004*\"nasa\" + 0.004*\"burger\" + 0.003*\"rahman\" + 0.003*\"google\" + 0.003*\"bow\" + 0.003*\"meat\"')\n",
      "(1, '0.012*\"football\" + 0.010*\"apple\" + 0.008*\"player\" + 0.007*\"audi\" + 0.007*\"game\" + 0.006*\"google\" + 0.006*\"space\" + 0.006*\"iphone\" + 0.005*\"goal\" + 0.005*\"ronaldo\"')\n",
      "(2, '0.011*\"player\" + 0.010*\"biryani\" + 0.009*\"google\" + 0.008*\"tennis\" + 0.006*\"atkinson\" + 0.004*\"hendrix\" + 0.004*\"dish\" + 0.003*\"game\" + 0.003*\"bean\" + 0.003*\"tournament\"')\n",
      "(3, '0.008*\"jackson\" + 0.007*\"apple\" + 0.007*\"hendrix\" + 0.006*\"film\" + 0.006*\"golf\" + 0.005*\"player\" + 0.005*\"iphone\" + 0.005*\"music\" + 0.004*\"pitt\" + 0.004*\"hockey\"')\n"
     ]
    }
   ],
   "source": [
    "#visualize model topics with word probabilities\n",
    "for top in lda.print_topics():\n",
    "    print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define word2vec model\n",
    "model = Word2Vec(final_refined_texts_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "##get the strings for the top 4 words for every topic\n",
    "topics = []\n",
    "threshold = 4\n",
    "for top in lda.print_topics():\n",
    "    topic_words = []\n",
    "    for temp in str(top[1]).split(\"+\"):\n",
    "        topic_words.append(temp[7:-1].replace('\"',''))\n",
    "    topics.append(topic_words[:threshold])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['audi', 'hamburger', 'noodle', 'space'],\n",
       " ['football', 'apple', 'player', 'audi'],\n",
       " ['player', 'biryani', 'google', 'tennis'],\n",
       " ['jackson', 'apple', 'hendrix', 'film']]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['space,hamburger', 'audi,apple', 'tennis,player', 'hendrix,jackson']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arghyadeep\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\Users\\Arghyadeep\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#get the best tags for every topic\n",
    "tags = []\n",
    "for topic in topics:\n",
    "    best_tag = \"\"\n",
    "    model_similarity = 0\n",
    "    for i in range(1,len(topic)):\n",
    "        for j in range(i):\n",
    "            #print(topic[i],topic[j])\n",
    "            if model.similarity(topic[i],topic[j]) > model_similarity:\n",
    "                model_similarity = model.similarity(topic[i],topic[j])\n",
    "                best_tag = topic[i] + \",\" + topic[j]\n",
    "    tags.append(best_tag)\n",
    "            \n",
    "print(tags)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arghyadeep\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "final_tags = []\n",
    "for tag in tags:\n",
    "    tag1 = str(tag.split(',')[0])\n",
    "    tag2 = str(tag.split(',')[1])\n",
    "    temp = [i[0] for i in model.most_similar(tag1)[:3]] + [j[0] for j in model.most_similar(tag2)[:3]]\n",
    "    final_tags.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nasa', 'mission', 'program', 'burger', 'chain', 'steak'],\n",
       " ['model', 'engine', 'quattro', 'announced', 'september', 'introduced'],\n",
       " ['tournament', 'player', 'playing', 'match', 'score', 'game'],\n",
       " ['guitar', 'experience', 'performance', 'michael', 'music', 'song']]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "loaded_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noodle,hamburger', 'player,football', 'tennis,player', 'apple,jackson']\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "for topic in topics:\n",
    "    best_tag = \"\"\n",
    "    model_similarity = 0\n",
    "    for i in range(1,len(topic)):\n",
    "        for j in range(i):\n",
    "            #print(topic[i],topic[j])\n",
    "            if topic[i] in loaded_model and topic[j] in loaded_model and loaded_model.similarity(topic[i],topic[j]) > model_similarity:\n",
    "                model_similarity = loaded_model.similarity(topic[i],topic[j])\n",
    "                best_tag = topic[i] + \",\" + topic[j]\n",
    "    tags.append(best_tag)\n",
    "            \n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arghyadeep\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "new_final_tags = []\n",
    "for tag in tags:\n",
    "    tag1 = str(tag.split(',')[0])\n",
    "    tag2 = str(tag.split(',')[1])\n",
    "    temp = [i[0] for i in loaded_model.most_similar(tag1)[:3]] + [j[0] for j in model.most_similar(tag2)[:3]]\n",
    "    new_final_tags.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['noodles', 'ramen', 'soba', 'burger', 'chain', 'steak'],\n",
       " ['players', 'playmaker', 'Player', 'rugby', 'game', 'code'],\n",
       " ['Tennis', 'volleyball', 'badminton', 'match', 'score', 'game'],\n",
       " ['apples', 'pear', 'fruit', 'michael', 'music', 'song']]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_final_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
